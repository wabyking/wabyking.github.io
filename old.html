<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01//EN" "http://www.w3.org/TR/html4/strict.dtd">
<html>
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8">
  <meta http-equiv="Content-Style-Type" content="text/css">
  <title>Benyou Wang's Homepage</title>
  <meta name="Generator" content="Cocoa HTML Writer">
  <meta name="CocoaVersion" content="1671.6">
  <style type="text/css">
  .center {
  margin: auto;
  width: 80%;
  /*border: 3px solid green;*/
  padding: 10px;
}

</style>
  <script async defer src="https://buttons.github.io/buttons.js"></script>
</head>
<body class="center">
<h1 style="margin: 0.0px 0.0px 16.1px 60.8px; text-indent: -60.8px; line-height: 28.0px; font: 24.0px Times; color: #000000; -webkit-text-stroke: #000000"><b>Benyou Wang</b></h1>
<p >Mail:   wabyking@gmail.com</p>

<p >Currently, I am an assistant professor in the Chinese University of Hong Kong, Shenzhen (CUHKSZ). I got my phd degree from the University of Padova, Italy (very fortune to be supervised by Massimo Melucci and Emanuele Di Buiccio). See our lab on <a href="https://freedomintelligence.github.io/"> CUHKSZ LLM group.</a> 
 </p>

<p> I submit my thesis in Sep. 30th 2021 （<a href="talks/PhD_Thesis_Draft.pdf" >Here</a> is a draft version）and have it defended in March 2022.</p>

 <p> I joined CUHKSZ as an assistant professor from June 1st 2022. Please send me emails if you are interested to work with me as a Ph.D., research associate, or post-doc. See JD <a href="talsk/jd-2024.pdf">here</a> </p>
 
<p ><b>News</b></p>
<ol class="ol1">
  <!-- <li>I am visiting Speech and Language Computing Group, Huawei Noah's Ark Lab, hosted by Lifeng Shang.</li> -->
  <li> 2024年入选腾讯犀牛鸟项目, CFF-滴滴盖亚学者项目 </li>
  <li> 2023年获得华为火花奖 </li>
  <li> I serve as a Website Chair in EMNLP 2023.</li>
  <li> I serve as a Publicity Chair in NLPCC 2023.</li>
  <li> Our paper (<a href= "https://arxiv.org/pdf/2207.09638.pdf"> Doge Ticket </a> )  got the Best Paper Award in NLPCC 2022, see <a href="images/NLPCC022.jpg"> here </a> .</li>
  <li> In September 2022, we got one paper accepted in NeurIPS (named <a href="papers/MorphTE-NeurIPS2022.pdf">MorphTE</a> ) and another paper in EMNLP (<a href="papers/acl_hypoformer.pdf">Hypoformer </a>). Both papers are about compressing transformer models (either in embedding or fully-connected layers) </li>
  <li> In August 2022, one paper got accepted in COLING, which extends deep prompt tuning (DPT) to dense retrieval. By using two additional strategies, DPT got comparable performance with a fine-tuning. </li>
  <li>A joking paper is released:  <a href="talks/mars.pdf"> Can we create a new creature? </a> </li>
  <li>A new paper is accepted in ICLR 2022 which could compress 12-layer BERT encoders into 1.5 M while  with slight performance drop, see <a href="https://openreview.net/forum?id=RftryyYyjiG">"Exploring extreme parameter compression for pre-trained language models".</a> </li>
  <li>Our paper titled  <a href="https://openreview.net/forum?id=vy9jsg8VyoG">"Word2Fun: Modelling Words as Functions for Diachronic Word Representation"</a> got accepted in NeurIPS 2021 with my supervisors Massimo and Emanuele. This introduces a new paradigm for time-specific word embeddings (e.g., imagining that word "president" in 2018 and 2021 generally refer to different people), with both theoretical advantages and empirical success. This is a kind of work that are smaller, better, and more interpretable.</li>
  <li>Our paper titled  <a href="https://openreview.net/forum?id=onxoVA9FxMw">"On position embeddings in BERT"</a> got accepted in ICLR 2021. Try <a href="https://google.com/search?q=position+embeddings" > searching "position embeddings" in Google </a>. </li>
  <li >Our paper titled  <a href="https://openreview.net/forum?id=Hke-WTVtwr">"Encoding word order in complex embeddings"</a> got accepted with a <b>spotlight</b> presentation in ICLR 2020 (acceptance rate 6%). <a href="https://github.com/iclr-complex-order/complex-order">Codes were already open-sourced.</a>. This is the first work for rotation-based position embeddings while the previous is translation-based. </li>
   <!-- <li ></li> -->
  <!-- <li >I was visiting  University of Montreal from Dec. 2019 to Feb. 2020, hosted by Prof. Jian-Yun Nie. Montreal is a nice city for life and research.</li> -->
  <!-- <li >I visited  University of Amsterdam for one week, hosted by Prof. Maarten de Rijke, to give a talk in the <a href="https://www.meetup.com/SEA-Search-Engines-Amsterdam/events/fxjbhryznbhc/"> Meetup Search Engines Amsterdam. </a> </li> -->
  <!-- <li >I was visiting DIKU IR Lab in University of Copenhagen from Sep. to Dec., 2019 hosted by Prof. Christina Lioma.</li> -->
  <li >We won the Best explainable NLP paper in NAACL 2019 with 1000 dollars, present our paper together with BERT authors (Best Long paper winner). <a href="images/NAACL2019.jpg"> here </a></li>
  <li> I got Marie Curry Fellowship to rejoin academia in 2018. Thanks for the generous funding.</li>
  <li>Our book <推荐系统与深度学习> has been published, buy it in  <a href="https://item.jd.com/12477453.html"> JD </a> </li>
  <li >We (IRGAN) won the Best Paper  honorable mention award in SIGIR 2017, one of the most-cited papers in SIGIR. See <a href="images/SIGIR2017.jpg"> here </a></li>

  <!-- <li >There is an open-sourced project called Ragu, targeting to smoothly generate fluent adversarial examples for text.</li> -->
  <!-- <li >I visited Tencent Quantum Lab, Tencent Cloud NLP team (Zhiwen Lab), Tianjin University and Toutiao Bytedance AI lab in June-July 2019.</li> -->
</ol>

<p ><b>Awards</b></p>
<ol class="ol1">
  <li ><a href="https://naacl2019.org/blog/best-papers/">NAACL 2019 Best Explainable NLP Paper</a></li>
  <li ><a href="http://sigir.org/sigir2017/program/awards/">SIGIR 2017 best paper award Honorable Mention</a></li>
  <li ><a href="http://www.quartz-itn.eu/people/esr/esr-2">Selected </a>as a Marie Curie Researcher of <a href="http://www.quartz-itn.eu/">Quantum Information Access and Retrieval Theory (QUARTZ)</a>, a fellowship for Early-stage Researcher, funded by the European Union's Horizon 2020 research and innovation program under the Marie Skłodowska-Curie grant agreement No. 721321</li>
  <!-- <li >International Collaboration Proposal Prize, 2019 Joint Statistics Summer School by Univeristy of Bolzano, Padova and Salzburg</li> -->
</ol>

<h3 style="margin: 0.0px 0.0px 0.0px 0.0px; line-height: 17.0px; font: 14.0px Times; color: #000000; -webkit-text-stroke: #000000"><b>Education</b></h3>
<ul class="ul1">
  <li >2018.10- 2022.3:  Ph.D student:  information engineering,      University of Padua, Italy.</li>
  <li >2014.9-2017.2:  Master:  pattern recognition and intelligent system,      Tianjin University, China.</li>
  <li >2010.9-2014.6:  Bachelor:  software engineer,      Hubei University of Automotive Technology, China.</li>
</ul>

<h3 style="margin: 0.0px 0.0px 0.0px 0.0px; line-height: 17.0px; font: 14.0px Times; color: #000000; -webkit-text-stroke: #000000"><b>Work and Intern</b></h3>
<ul class="ul1">
  <li >2022.6-~: Assistant Professor, School of Data Science, the Chinese University of Hong Kong, Shenzhen. </li>
  <li >2018.6-2021.6:  Marie Curie Researcher, Department of Information Engineering, University of Padua, Italy. </li>
  <li >2020.12:  Visiting Scholar. Institute of Theoretical Physics, Chinese Academy of Science (CAS) in Dec. 2020 hosted by Pan Zhang </li>
  <li >2019.11-2020-2:  Visiting Student.  University of Montreal from Dec. 2019 to Feb. 2020, hosted by Prof. Jian-Yun Nie. </li>
  <li >2019.10:  Visiting Student (one week).  University of Amsterdam, hosted by Prof. Maarten de Rijke. </a> </li>
  <li >2019.9-2019.12:  Visiting Student.  DIKU IR Lab in University of Copenhagen, hosted by Prof. Christina Lioma. </li>
  <li >2017.7-2018.6:  Full-time Associate Researcher, Data Application Center, Tencent, China. </li>
  <!-- <li >2017.5-2017.7:  Research Consultant, Input method group, Sougou, China.</li> -->
  <!-- <li >2017.2-2017.5:  Research Assistant, Laboratory of Cognitive Computing and Application, Tianjin University.</li> -->
  <!-- <li >2015.8--2015.9 &amp; 2016.4-2016.5  Research Intern, <a href="http://ling.cass.cn/">Institute of Linguistics </a>, <a href="http://casseng.cssn.cn/">China Academy of Social Sciences </a>Beijing, China.</li> -->
</ul>

<h3 style="margin: 0.0px 0.0px 0.0px 0.0px; line-height: 17.0px; font: 14.0px Times; color: #000000; -webkit-text-stroke: #000000"><b>Professional Activities</b></h3>
<ul class="ul1">
  <li> Website Chair of EMNLP 2023 </li>
  <li> Publicity Chair of NLPCC 2023 </li>
  <li > PC member of  <a href="https://diacr-ita.github.io/DIACR-Ita/">  the workshop of Diachronic Lexical Semantics  evaluation task  </a>  in  <a href="http://www.evalita.it/2020/"> the 7th evaluation campaign of Natural Language Processing and Speech tools for Italian</a>  </li>
  <li > PC member of  <a href="https://www.aclweb.org/anthology/2020.eval4nlp-1.0.pdf"> the First Workshop  Evaluation and Comparison of NLP Systems in EMNLP 2020</a>  </li>
  <li > We co-organized <a href="kingmon_workshop.html"> the Kingston-Montreal NLP/IR workshop </a>  between researchers in RALI lab, University of Montreal and Queen's University. </li>
  <li >Founding member of Quantum Penguin Club in Tencent, the predecessor of Tencent Quantum Lab.</li>
  <li >Serveing as an Executive Committee of <a href="http://www.ccf.org.cn/">CCF (China computer Federation) </a>Tianjin University Branch in 2015-2016. I am responsible for inviting scholars for academic communication.</li>
  <li >Reviewer : ICLR 2021/2020, NeurIPS 2021/2020, ICML 2022</li>
  <!-- <li >Attendied conference: ACL 2019, SIGIR 2019, WWW 2019, NAACL 2019, ICML 2018, IJCAI 2018, CIKM 2018, ICTIR 2018</li> -->
</ul>

<h3 style="margin: 0.0px 0.0px 0.0px 0.0px; line-height: 17.0px; font: 14.0px Times; color: #0000e3; -webkit-text-stroke: #0000e3"><b><a href="publications.html" target="_blank">Publications </b><a href="https://scholar.google.com/citations?user=Jk4vJU8AAAAJ"><b>@ Google Scholar</b></a><b> </b></h3>
<h3 style="margin: 0.0px 0.0px 0.0px 0.0px; text-align: justify; line-height: 14.0px; font: 12.0px Times; color: #000000; -webkit-text-stroke: #000000"><b> After being a faculty</b>. <u> Phd students </u> and <u> Research Assistants </u> under my supervision are underlined </h4>
  <ol class="ol1">

<li> <u> Juhao Liang </u>, <u> Zhenyang Cai </u> , Jianqing Zhu, Huang Huang, Kewei Zong, Bang An, Mosen Alharthi, Juncai He, Lian Zhang, Haizhou Li, Benyou Wang#, Jinchao Xu. Alignment at Pre-training! Towards Native Alignment for Arabic LLMs. <b> NeurIPS 2024  </b>  </li>

<li> Pengcheng Chen, Jin Ye, Guoan Wang, Yanjun Li, Zhongying Deng, Wei Li, Tianbin Li, Haodong Duan, Ziyan Huang, Yanzhou Su, Benyou Wang, Shaoting Zhang, Bin Fu, Jianfei Cai, Bohan Zhuang, Eric J Seibel, Junjun He, Yu Qiao. GMAI-MMBench: A Comprehensive Multimodal Evaluation Benchmark Towards General Medical AI. <b> NeurIPS 2024 Track Datasets and Benchmarks  </b>   </li>


<li> Qianqian Xie, Weiguang Han, Zhengyu Chen, Ruoyu Xiang, Xiao Zhang, Yueru He, Mengxi Xiao, Dong Li, Yongfu Dai, Duanyu Feng, Yijing Xu, Haoqiang Kang, Ziyan Kuang, Chenhan Yuan, Kailai Yang, Zheheng Luo, Tianlin Zhang, Zhiwei Liu, GUOJUN XIONG, Zhiyang Deng, Yuechen Jiang, Zhiyuan Yao, Haohang Li, Yangyang Yu, Gang Hu, Huang Jiajia, Xiao-Yang Liu, Alejandro Lopez-Lira, Benyou Wang, Yanzhao Lai, Hao Wang, Min Peng, Sophia Ananiadou, Jimin Huang.    
FinBen: An Holistic Financial Benchmark for Large Language Models.  <b> NeurIPS 2024 Track Datasets and Benchmarks  </b>  </li>

<li> <u> Junying Chen </u>, <u> Chi Gui </u>,  <u> Ruyi Ouyang </u>, Anningzhe Gao, <u> Shunian Chen </u>, <u> Guiming Hardy Chen </u>, <u> Xidong Wang </u>, <u> Ruifei Zhang </u>, <u> Zhenyang Cai </u>, Ke Ji, Guangjun Yu, Xiang Wan, Benyou Wang. HuatuoGPT-Vision, Towards Injecting Medical Visual Knowledge into Multimodal LLMs at Scale. <b> EMNLP 2024.  </li>

<li> <u> Guiming Hardy Chen</u>, <u> Shunian Chen</u>, <u> Ziche Liu</u>, Feng Jiang, Benyou Wang. Humans or LLMs as the Judge? A Study on Judgement Biases. <b> EMNLP 2024  </b>   </li>

<li> Lei Li, Zhihui Xie, Mukai Li,<u> Shunian Chen</u>, Peiyi Wang, Liang Chen, Yazheng Yang, Benyou Wang, Lingpeng Kong, Qi Liu. VLFeedback: A Large-Scale AI Feedback Dataset for Large Vision-Language Models Alignment.  <b> EMNLP 2024  </b>   </li>



<li> <u>Junying Chen</u>, <u>Xidong Wang</u>, <u>Ke Ji</u>, Anningzhe Gao, Feng Jiang, <u>Shunian Chen</u>,<u> Hongbo Zhang</u>, <u>Dingjie Song</u>, <u>Wenya Xie</u>, <u>Chuyi Kong</u>, <u>Jianquan Li</u>, Xiang Wan, Haizhou Li, Benyou Wang. HuatuoGPT-II, One-stage Training for Medical Adaption of LLMs. <b> COLM 2024  </b>  </li>

<li>  <u>Song Dingjie</u>, <u>Shunian Chen</u>, <u>Guiming Hardy Chen</u>, <u>Fei Yu</u>, Xiang Wan, Benyou Wang. MileBench: Benchmarking MLLMs in Long Context. <b> COLM 2024  </b>   </li>


<li> <u>Chen Zhang</u>, Benyou Wang, Dawei Song. On Elastic Language Models. <b> ACM TOIS </b>  </li>


<li> <u> Zhengyang Tang</u>, Xingxing Zhang, <b>Benyou Wang</b>, Furu Wei.  MathScale: Scaling Instruction Tuning for Mathematical Reasoning <b> ICML 2024 </b> </li>

    
 <li>Chuyi Kong, Yaxin Fan, Xiang Wan, Feng Jiang, and Benyou Wang. Large Language Model as a User Simulator. <a href="https://arxiv.org/abs/2308.11534">  ACL 2024 </a> </li>
<li> <u> Zhengyang Tang</u>, Xingxing Zhang, <b>Benyou Wang</b>, Furu Wei.  MathScale: Scaling Instruction Tuning for Mathematical Reasoning <b> ICML 2024 </b> </li>

<li> <u> Xianghong Fang</u>, Jian Li, Qiang Sun, <b>Benyou Wang</b>. Rethinking the Uniformity Metric in Self-Supervised Learning. <b> ICLR 2024 </b> </li>

<li> <u> Xidong Wang </u>, <u>Guiming Hardy Chen</u>, <u>Dingjie Song</u>, <u>Zhiyi Zhang</u>, <u>Zhihong Chen</u>, Qingying Xiao, Feng Jiang, <u>Jianquan Li</u>, Xiang Wan, <b> Benyou Wang</b>, Haizhou Li. CMB: A Comprehensive Medical Benchmark in Chinese. 2023. <a href="https://arxiv.org/abs/2308.08833"> NAACL 2024 </a>  &nbsp  <a href="https://cmedbenchmark.llmzoo.com/"> Online leaderboard </a> </li>


<li>  <u>Huang Huang</u>, <u>Fei Yu</u>, Jianqing Zhu, <u>Xuening Sun</u>, Hao Cheng, <u>Dingjie Song</u>, <u>Zhihong Chen</u>, Abdulmohsen Alharthi, Bang An, Juncai He, <u>Ziche Liu</u>, <u>Zhiyi Zhang</u>, <u>Junying Chen</u>, <u>Jianquan Li</u>, <b>Benyou Wang</b>, Lian Zhang, Ruoyu Sun, Xiang Wan, Haizhou Li, Jinchao Xu. AceGPT, Localizing Large Language Models in Arabic. NAACL 2024.  (<a href="https://huggingface.co/FreedomIntelligence/AceGPT-7B"> HuggingFace </a> downloading: 10K per month.)  </li>


<!-- <li>Ridong Han, Tao Peng, Chaohao Yang, <b> Benyou Wang</b> Lu Liu, and Xiang Wan. Is Information Extraction Solved by ChatGPT? An Analysis of Performance, Evaluation Criteria, Robustness and Errors. 2023. <a href="https://arxiv.org/abs/2305.14450"> Arxiv. </a> </li> -->

<!-- <li>Jianquan Li, Xidong Wang, Xiangbo Wu, Zhiyi Zhang, Xiaolong Xu, Jie Fu, Prayag Tiwari, Xiang Wan, and <b>Benyou Wang</b>. Huatuo-26M, a Large-scale Chinese Medical QA Dataset. 2023. <a href="https://arxiv.org/abs/2305.01526"> Arxiv. </a> </li> -->

<li> <u>Fei Yu</u>, Anningzhe Gao, <b>Benyou Wang</b>. Outcome-supervised verifiers for planning in mathematical reasoning. Findings of NAACL 2024.  (The work brings 7B LLMs to the era with an accuracy of 0.8 and even 0.9  in GSM8K, see <a href="https://paperswithcode.com/sota/arithmetic-reasoning-on-gsm8k" > the leaderboard </a> ) </li>

<li><u> Hongbo Zhang</u>, <u> Junying Chen</u>, Feng Jiang, <u>Fei Yu</u>, <u>Zhihong Chen</u>, <u>Jianquan Li</u>, <u>Guiming Chen</u>, <u>Xiangbo Wu</u>, <u>Zhiyi Zhang</u>, Qingying Xiao, Xiang Wan, <b> Benyou Wang</b> Haizhou Li. HuatuoGPT, towards Taming Language Model to Be a Doctor. 2023. <a href="https://arxiv.org/abs/2305.15075"> Findings of EMNLP 2023 </a> (GitHub stars: 1K; <a href="https://www.huatuogpt.cn/"> Online access </a> : 400K+  ) ) </li>


<li> <u>Zhihong Chen</u>, Feng Jiang, Junying Chen, Tiannan Wang, Fei Yu, Guiming Chen, Hongbo Zhang, Juhao Liang, Chen Zhang, Zhiyi Zhang, Jianquan Li, Xiang Wan, <b> Benyou Wang </b>, Haizhou Li. "Phoenix: Democratizing chatgpt across languages".  <a href="https://arxiv.org/pdf/2304.10453"> Arxiv </a> <a href="https://github.com/FreedomIntelligence/LLMZoo"> code (Github stars: 3K) </a> &nbsp; <a href="https://huggingface.co/FreedomIntelligence/phoenix-inst-chat-7b"> HuggingFace downloading: 4K per month </a>  </li>



<li><u> Fei Yu</u>, <u>Hongbo Zhang</u>, Prayag Tiwari, and <b>Benyou Wang</b>. Natural language reasoning, a survey. 2023. <a href="https://arxiv.org/abs/2303.14725"> ACM Computing Survey. </a> </li> 


<li> Zhongwei Wan, Che Liu, Mi Zhang, Jie Fu, <b>Benyou Wang</b>, Sibo Cheng, Lei Ma, César Quilodrán-Casas, Rossella Arcucci. Med-UniC: Unifying Cross-Lingual Medical Vision-Language Pre-Training by Diminishing Bias. <b>NeurIPS  2023 </b> </li>
<li> Yazhou Zhang, Yang Yu, Qing Guo, <b> Benyou Wang </b>, Dongming Zhao, Sagar Uprety, Dawei Song, Jing Qin, Qiuchi Li.  All In One: A Chinese Multi-Modal Dataset for Multi-Affection Detection in Conversations. <b>NeurIPS 2023  Track Datasets and Benchmarks </b> </li>
<li> <u> Zhihong Chen</u>, Shizhe Diao, <b> Benyou Wang</b>, Guanbin Li, and Xiang Wan. <a>"Towards Unifying Medical Vision-and-Language Pre-training via Soft Prompts".</a>   <b>ICCV 2023 </b> </li>
<li> Zhongwei Wan, Xin Liu, <b>Benyou Wang</b>, Jiezhong Qiu, Boyu Li, Ting Guo, Guangyong Chen, Yang Wang. Spatio-Temporal Contrastive Learning Enhanced GNNs for Session-based Recommendation. <b>Transactions on Information Systems (TOIS) </b> </li>   
<li> <u>Jianquan Li</u>, <u> Xiangbo Wu </u>, <u> Xiaokang Liu </u>,  Prayag Tiwari, Qianqian Xie, and <b>Benyou Wang</b>. <a>"Can Language Models Make Fun? A Case Study in Chinese Comical Crosstalk".</a>   <b>ACL 2023 </b> </li>

<li> <u> Yajiao LIU </u> , Xin Jiang, Yichun Yin, Yasheng Wang, Fei Mi, Qun Liu, Xiang Wan, and <b>Benyou Wang</b>. One Cannot Stand for Everyone! Leveraging Multiple User Simulators to train Task-oriented Dialogue Systems. <b>ACL 2023 </b> </li>

<li> <u> Chen Zhang </u>, Yang Yang, Jiahao Liu, Jingang Wang, Yunsen Xian, <b> Benyou Wang</b> and Dawei Song. Lifting the Curse of Capacity Gap in Distilling Language Models. <b>ACL 2023 </b> </li>


<li> <u> Zhihong Chen </u>, <u> Guiming Hardy Chen </u>, Shizhe Diao, Xiang Wan, and <b>Benyou Wang</b>. On the Difference of BERT-style and CLIP-style Text Encoders. <b>Findings  of ACL 2023 </b> </li>


<li> <u> Xiaokang Liu </u>, <u> Jianquan Li </u>, Jingjing Mu, Min Yang, Ruifeng Xu, and <b>Benyou Wang</b>. <a>"Effective Open Intent Classification with K-center Contrastive Learning and Adjustable Decision Boundary".</a>   <b>AAAI  2023 </b> </li>
<li>Le Sun, Mingyang Zhang, <b> Benyou Wang</b> and Prayag Tiwari. Few-Shot Class-Incremental Learning for Medical Time Series Classification. <a> IEEE Journal of Biomedical and Health Informatics </a>. 2023 </li>

<li>Yaochen Liu, Qiuchi Li, <b> Benyou Wang</b> Yazhou Zhang, Dawei Song. <a>"A survey of quantum-cognitively inspired sentiment analysis models".</a>   <b>ACM Computing Surveys 2023 </b> </li>


<li > <b>Benyou Wang</b>, Qianqian Xie, Jiahuan Pei, Prayag Tiwari, Zhao Li, and Fu Jie. <a href="https://dl.acm.org/doi/pdf/10.1145/3611651">"Pre-trained Language Models in Biomedical Domain: A Survey from Multiscale Perspective".</a>   <b>ACM Computing Surveys, </b>. </li>
<li > Yi Yang, Chen Zhang, <b> Benyou Wang</b> and Dawei Song. <a href="https://arxiv.org/abs/2207.09638">"Doge Tickets: Uncovering Domain-general Language Models by Playing Lottery Tickets".</a>  <a  style="color: red"> <b>NLPCC 2022 Best Paper 2022 </b> </a>. </li>
<li > <u> Sunzhu Li </u>, Peng Zhang, Guobing Gan, Xiuqing Lv, <b> Benyou Wang</b> Junqiu Wei, Xin Jiang. <a>"Hypoformer: Hybrid Decomposition Transformer for Edge-friendly Neural Machine Translation".</a>   <b>EMNLP 2022 </b>. </li>
<li > Guobing Gan, Peng Zhang, Sunzhu Li, Xiuqing Lu, <b> Benyou Wang</b>. <a>"MorphTE: Injecting morphology in tensorized embeddings".</a>   <b>NeurIPS 2022 </b>. </li>
  </ol>
  
<h3 style="margin: 0.0px 0.0px 0.0px 0.0px; text-align: justify; line-height: 14.0px; font: 12.0px Times; color: #000000; -webkit-text-stroke: #000000"><b> Before being a faculty</b></h4>

<ol class="ol1">
  <li > <b>Benyou Wang</b>, Yuxin Ren, Lifeng Shang, Xin Jiang, Qun Liu. <a href="https://openreview.net/forum?id=RftryyYyjiG">"Exploring extreme parameter compression for pre-trained language models".</a>   <b>ICLR 2022, </b>. </li>
  <li > Peng Zhang, Wenjie Hui, <b>Benyou Wang</b> (corresponding), Donghao Zhao, Dawei Song, Christina Lioma, Jakob Grue Simonsen. <a href=https://dl.acm.org/doi/10.1145/3505138">"Complex-valued Neural Network-based Quantum Language Models".</a>   <b>ACM Transactions on Information Systems </b>. </li>
  <li > <b>Benyou Wang</b>, Emanuele Di Buiccio, Massimo Melucci. <a href="https://openreview.net/forum?id=vy9jsg8VyoG4"> Word2fun, modeling words as functions for dynamic word embeddings. </a>   <b>NeurIPS 2021, </b>. </li>
  <li > <b>Benyou Wang</b>, Lifeng Shang, Christina Lioma, Xin Jiang, Qun Liu, Jakob Grue Simonsen.<a href="https://openreview.net/forum?id=onxoVA9FxMw"> On position embeddings in BERT.</a>  <b>ICLR 2021, </b> </li>
  <li > <b>Benyou Wang</b>*, Donghao Zhao*, Christina Lioma, Qiuchi Li, Peng Zhang, Jakob Grue Simonsen. Encoding word order in complex embeddings. <b>ICLR 2020, </b><a href="https://openreview.net/forum?id=Hke-WTVtwr", style="color: red"><b>Spotlight paper</b></a> (acceptance rate: 6%) </li>
  <li >Qiuchi Li*, <b>Benyou Wang</b>*, Massimo Melucci. A Complex-valued Network for Matching. <b>NAACL 2019, </b><a href="https://naacl2019.org/blog/best-papers/" style="color: red"><b>Best Explainable NLP Paper</b></a></li>
  <li ><b></b><b>Benyou Wang</b>. Dynamic content monitoring and exploration using vector spaces. <b>SIGIR 2019 </b>doctoral consortium. </li>
  <li ><b></b><b>Benyou Wang</b>*, Qiuchi Li*, Massimo Melucci, Dawei Song. <a href="papers/www2019.pdf">Semantic Hilbert Space for Text Representation Learning.</a> <b>WWW 2019</b></li>
  <li >Wei Zhao*, <b>Benyou Wang</b>*, Min Yang, Jianbo Ye, Zhou Zhao, Xiaojun Chen, Ying Shen.. <a href="papers/toc.pdf">Leveraging Long and Short-term Information in Content-aware Movie Recommendation via Adversarial Training.</a> <b>IEEE Transactions on Cybernetics (TOC), 2019</b> (IF: 8.803)</li>
 
  <li >Peng Zhang, Zhan Su, Lipeng Zhang, <b>Benyou Wang </b>, Dawei Song. 2018. <a href="papers/QMBF.pdf">A Quantum Many-body Wave Function Inspired Language Modeling Approach.</a> <b>CIKM 2018</b></li>


  <li >Wei Zhao, <b>Wang Benyou </b>, Jianbo Ye, Yongqiang Gao, Min Yang, Xiaojun Chen, PLASTIC: Prioritize Long and Short-term Information in Top-n Recommendation using Adversarial Training, <b>IJCAI 2018</b></li>
  <li >Wei Zhao, <b>Wang Benyou </b>, Jianbo Ye, Min Yang, Zhou Zhao, Ruotian Luo, Yu Qiao A Multi-task Learning Approach for Image Captioning, <b>IJCAI 2018</b></li>
  
  <li >Zhang Peng, Niu Jiabing, Su Zhan, <b>Wang Benyou </b>et al. <a href="papers/aaai-2018.pdf">End-to-End Quantum-like Language Models with Application to Question Answering </a><b>AAAI 2018 </b></li>


  <li >Wang Jun, Yu Lantao, Zhang Weinan, Gong Yu, Xu Yinghui, <b>Wang Benyou</b> , Zhang Peng, Zhang Dell. <a href="https://arxiv.org/pdf/1705.10513.pdf">IRGAN: A Minimax Game for Unifying Generative and Discriminative Information Retrieval Models. </a><b>SIGIR 2017. </b><a href="http://sigir.org/sigir2017/program/awards/" style="color: red"><b>Best Paper Award Honourable Mentions</b><b> </b></a>. <a href="https://zhuanlan.zhihu.com/p/27297452">Zhihu link (in Chinses)</a></li>


  <li ><b></b><b>Wang Benyou</b>, Niu Jiabing, Ma Liqun, Zhang Yuhua, Zhang Lipeng, Li Jinfei, Zhang Peng Song, D. . <a href="https://link.springer.com/chapter/10.1007/978-3-319-50496-4_88">A Chinese Question Answering Approach Integrating Count-Based and Embedding-Based Features. </a>ICCPOL-NLPCC . December, 2016</li>
  <li ><b></b><b>Wang Benyou</b>, Zhang Peng, Li Jinfei, Song Dawei, Hou Yuexian, Shang Zhenguo. <a href="http://www.mdpi.com/1099-4300/18/4/144/pdf">Exploration of quantum interference in document relevance judgement discrepancy. </a><b>Entropy</b> , 18(4), 144. 2016. (IF : 1.821)</li>


  <li >Chen Yongqiang, Zhang Peng, Song Dawei, <b>Wang Benyou</b>. <a href="https://dl.acm.org/citation.cfm?id=2806602">A Real-Time Eye Tracking Based Query Expansion Approach via Latent Topic Modeling. </a><b>CIKM 2015 </b>(pp. 1719-1722). ACM. October, 2015</li>
</ol>
<h4 style="margin: 0.0px 0.0px 0.0px 0.0px; text-align: justify; line-height: 14.0px; font: 12.0px Times; color: #000000; -webkit-text-stroke: #000000"><b>Book and book chapter</b></h4>
<ol class="ol1">
  <li >Huang Xin, Wei zhao, <b>Wang Benyou</b>, Rui Zhao. Recommendation System and Deep Learning, Tsinghua University Press, in Chinese. Focusing on the chapters related "Learn to rank" and "Generative Adversarial Nets(GAN) for Recommendation". Online purchase link: <a href="https://item.jd.com/12477453.html">JD</a> and <a href="http://product.dangdang.com/26439080.html">Dangdang</a>. </li>
  <li ><b></b><b>Wang,B. </b>, Emanuele Di, B., &amp; Melucci, M.. <a href="papers/Wang2019_Chapter_RepresentingWordsInVectorSpace.pdf">  Representing words in vector space and beyond </a>. In A. Diederik, K. Andrei, M. Massimo, &amp; T. Bourama (Eds.),Quantum-like models forinformation retrieval and decision-making. Springer. </li>
</ol>

<h3 style="margin: 0.0px 0.0px 0.0px 0.0px; text-align: justify; line-height: 17.0px; font: 14.0px Times; color: #000000; -webkit-text-stroke: #000000"><b>Talks</b></h3>
<ol class="ol1">
   
   <li> "Discussion on the border of LLMs", the Practice from HuatuoGPT, 大模型机理分析论坛, July 20th-30th 2024, Taian, Shandong, China Conference on Data Mining (CCDM 2024), invited by Prof. Chenliang Li</li>
   <li> "Large Language Models for Healthcare", the Practice from HuatuoGPT, YSSNLP, Kunming, Yunnan, June 14th 2024, invited by Prof. Chenliang Li</li>
  <li>The practice and thinking of medical LLMs,  大模型赋能智能医疗的 workshop, May 6th 2024, Chongqing, invited by Prof. Hao Chen  </li>
  <li><a href="#">"The progress of HuatuoGPT",  Guest lecture, Soutech, May 2024. invited by Prof. Bingyi Jin</a></li>
   <li><a href="#"> "The present and the future of large language models", The campus open day. Apirl 2024</a></li>
   <li><a href="#">"The progress of the medical LLM HuatuoGPT", April 2024, Zhejiang University, invited by Prof. Guangyong Chen</a></li>
   <li><a href="#">"The progress of the medical LLM HuatuoGPT", April 2024, Shanghai AI Lab, invited by Dr. Huaxi Huang</a></li>
   <li><a href="#">"The progress of the medical LLM HuatuoGPT", April 2024, Peking University, Prof. Xiaohua Zhou</a></li>
   <li><a href="#">"The practice and thinking of large langauge models", March 2024, Nankai Univesity, invited by Prof. Jie Liu</a></li>
   <li><a href="#">"The progress of HuatuoGPT", HKUST (Guangzhou), April 2024, invited by Prof. Li Liu</a></li>
    <li><a href="#">"Large language models, the practice and the future", Huawei 诺亚大讲堂, hosted by Dr. Lifeng Shang,  2024</a></li>
    <li><a href="#">"What can quantum physics bring to NLP", invited talk by Prof. Masahito Hayasi in the Japan-China Joint Workshop on Quantum Information, September 2023.</a></li>
  <li><a href="#">"AceGPT, the SOTA Arabic LLM", KAU talk invited by Prof. Eman, September 2023.</a></li>
  <li><a href="#">"Medical LLM, the practice from HuatuoGPT", online talk invited by Prof. Libo Qin in MLNLP, September 2023.</a></li>
  <li><a href="#">"Medical LLM, the practice from HuatuoGPT", CIPS tutorial in Jinan, September 2023.</a></li>
  <li><a href="#">"HuatuoGPT: taming language models to be doctors", IJCAI workshop hosted by Prof. Chen Chen, August 2023.</a></li>
  <li><a href="#">"Medical LLMs", CCF (中国计算机协会术语委员会), hosted by Prof. Peng Zhang, August 2023.</a></li>
  <li><a href="#">"What should we do in the LLM era", SRIBD to undergraduate students, hosted by the SRIBD Director Ping Lee, August 2023.</a></li>
  <li><a href="#">"LLM practice", KAUST, hosted by Prof. Jinchao Xu, August 2023.</a></li>
  <li><a href="#">"Vertical LLM", CIPS (中文信息学会讲习班), hosted by Prof. Min Yang, June 2023.</a></li>
  <li><a href="#">"Medical ChatGPT, our practice", HK Hospital Authority (香港医管局), May 2023.</a></li>
  <li><a href="#">"Medical ChatGPT, our practice", CCF中国计算机协会天津分会, hosted by Prof. Peng Zhang, May 2023.</a></li>
  <li><a href="#">"Large language model, our practice", KAUST, hosted by Prof. Jinchao Xu, May 2023.</a></li>
  <li><a href="#">"The interplay between Large language model and IR", Huawei 大讲堂, hosted by Prof. Rui Zhang, May 2023.</a></li>
  <li><a href="#">"Introduction to ChatGPT", undergraduate students in SDS, Hosted by Prof. Hongyuan Zha, April 2023.</a></li>
  <li><a href="#">"Introduction to ChatGPT", CIPS中文信息学会青工委, Hosted by Prof. Chenliang Ma, March 2023.</a></li>
  <li><a href="#">"Introduction to ChatGPT", Shanghai Jiaotong University, Hosted by Prof. Weinan Zhang, March 2023.</a></li>
  <li><a href="#">"Vertical LLM", Shenzhen Robot and AI lab, hosted by Prof. Hongyuan Zha, November 2022.</a></li>
  <li><a href="#">"Some proposals on biomedical NLP", online seminar, hosted by Prof. Xiang Wan, SRIBD, September 2022.</a></li>
  <li><a href="#">"Position embeddings", AI Times, hosted by Miss He Yun, July 2022.</a></li>
  <li><a href="https://weibo.com/1107613433/Lxo2AFEay">"How and whether AI replaces Humans", Weibo live streaming with more than 1M watching, hosted by Mr Qingyi Gao</a>, June 2022.</li>
  <li><a href="#">"Position embeddings", Renmin University of China, hosted by Prof. Yong Liu, April 2022.</a></li>
  <li><a href="#">"Quantum and AI: Opportunities, Challenges, and Future Trends", Huawei, Shenzhen, hosted by Prof. Qun Liu, March 2022.</a></li>
  <li><a href="#">"How quantum physics contributes to NLP", HIT(SZ), Shenzhen, hosted by Prof. Zenglin Xu, December 2021.</a></li>
  <li><a href="#">"Word2Fun, modeling words as functions for diachronic word embeddings", CSIRO, Australia, online, December 2021.</a></li>
  <li><a href="#">"Large-scale Pre-trained Language Models (PLMs): Potentials, Efficiency, and Future Trends", SUSTech, Shenzhen, hosted by Qi-Man Shao, December 2021.</a></li>
  <li><a href="#">"Bridging Quantum physics and NLP", Microsoft Research Asia (MSRA), Beijing, online, hosted by Nan Duan, November 2021.</a></li>
  <li><a href="#">"How physics and NLP help each other?", Institute of Theoretical Physics, Chinese Academy of Science (CAS) Beijing, December 2020.</a></li>
  <li><a href="#">"On Position embeddings", Alibaba, Beijing, China, December 2020.</a></li>
  <li><a href="#">"How quantum theory contributes to NLP", First workshop of quantum computing and AI, virtually, previously in Tianjin University, Tianjin, China, December 2020.</a></li>
  <li><a href="#">"Encoding Word Order in Complex Embeddings", Speech and Language Computing Group, Huawei Noah's Ark Lab, Shenzhen, China, December 2020.</a></li>
  <li><a href="#">"Investigating complex-valued representation in NLP", Montreal Institute for Learning Algorithms (MILA), Montreal, Canada, hosted by MILA NLP reading group (hosted by Alessandro Sordoni and Siva Reddy), January 21st, 2020.</a></li>
  <li><a href="#">"Beyond particles: modeling words as waves", RALI Département d’informatique et recherche opérationnelle, University of Montreal, Montreal, Canada, hosted by Jian-Yun Nie, December 7th, 2019.</a></li>

    <li ><a href="talks/iir2021_slides.pdf"> Sequencial Modeling in Vector Spaces</a>, the Italian Information Retreival Workshop, in Sep 2021</li>
    <li ><a href="#"> On Position embeddings </a>, the China Student Symposium on NLP (CSSNLP), Beijing, in December,  2020</li>
    <li ><a href="#"> Invited lecture: Quantum theory and NLP </a>,  for bachelor  students, Beijing Institute of Technolohy, Beijing, in December,  2020</li>
    <li ><a href="#"> Invited lecture: pretrained language model and its position embeddings </a> for bachelor  students, Shandong University, Qingdao, in Dec.  2020</li>
    <li ><a href="#"> How physics and NLP help each other? </a>, <a href="http://english.itp.cas.cn/au/">  Institute of theoretical Physics, Chinese Academy of Science (CAS)  </a> Beijing, in December,  2020</li>
    <li ><a href=""> On Position embeddings </a>, Alibaba, Beijing in December,  2020</li>
    <li ><a href="talks/UNIPD@DIACR-Ita.pdf"></a>, Formulizing semantic shift detection as a distance between sets<a href="http://ceur-ws.org/Vol-2765/"> EVALITA 2020. Seventh Evaluation Campaign of Natural Language Processing and Speech Tools for Italian</a> <a href="https://diacr-ita.github.io/DIACR-Ita/">  Diachronic Lexical Semantics  </a>  online, on December 17th  2020</li>
    <li ><a href="talks/QCAI-QCAI.pdf">How quantum theory contributes to NLP</a>, <a href="https://qcai2020.gitee.io/"> First workshop of quantum computing and AI</a>,  virtually, previously in Tianjin University, Tianjin, China, 22. Nov. 2020</li>
    <li ><a href="talks/noah-talk.pdf">Encoding word order in complex embeddings</a>, <a href="http://www.noahlab.com.hk/#/home"> Speech and Language Computing Group, Huawei Noah's Ark Lab</a>, Shenzhen, China, 23. April 2020</li>
  <li ><a href="talks/quartz-uk.pdf">Dynamic Content Monitoring and Exploration using Vector Spaces</a>, <a href="https://www.beds.ac.uk/howtoapply/departments/computing"> University of Bedfordshire</a>, Luton, Lonton, UK, 12 Feb. 2020</li>
  <li ><a href="talks/uk-meetup.pdf">Quantum Mechanics meet Information Search and Retrieval – The QUARTZ Project</a>, <a href="https://www.meetup.com/textanalytics/"> Great London Text Analytics meetup</a>, London, UK, 12. Feb. 2020</li>
  <li ><a href="talks/mila-talk.pdf">Investigating complex-valued representation in NLP</a>, <a href="https://mila.quebec/en/"> Mila</a>, Mila, Montreal, Canada, 20. Jan 2020</li>
  <li ><a href="talks/montreal-talk.pdf">Beyond particles: modeling words as waves </a>, <a href="http://rali.iro.umontreal.ca/rali"> RALI Département d’informatique et recherche opérationnelle</a>, University of Montreal, Montreal, Canada, Dec. 7th 2019</li>
  <li ><a href="talks/cph-talk.pdf">Beyond particles: modeling words as waves </a>, <a href="https://di.ku.dk/english/research/machine-learning/"> DIKU machine learning section</a>, University of Copenhagen, Copenhagen, Denmark, Nov. 25th 2019</li>
  <li ><a href="talks/ams-talk-short.pdf">Quantum formulations for language: understand words as particles </a>, <a href="https://www.meetup.com/SEA-Search-Engines-Amsterdam/events/fxjbhryznbhc/"> meetup Search Engines Amsterdam </a>, University of Amsterdam, Amsterdam. Netherlands, Oct. 25th 2019</li>
  <li ><a href="talks/sigir-dc-final.pdf">Dynamic Content Monitoring and Exploration using Vector Spaces </a>, SIGIR Doctoral Consortium, Paris, France. July, 2019</li>
  <li >2019 Joint Statistics Summer School by Univeristy of Bolzano, Padova and Salzburg, Brixen, Italy. July 11, 2019</li>
  <li ><a href="talks/toutiao.pdf"> Quantum-inspired NLP/IR </a>. Bytedance AI Lab, Hang Li's group, Beijing, China. June 28, 2019</li>
  <li >Quantum-inspired NLP/IR. Tencent Cloud NLP team (Zhiwen Lab), Shenzhen, China, June 21, 2019</li>
  <li ><a href="talks/quantumNLP.pdf">Representing and interpreting words in vector space inspired by Quantum theory.</a> Quartz Workshop, University of Copenhagen</li>
  <li ><a href="talks/DL_tensor.pdf">Tensor analysis for DL.</a> Functional Analysis, University of Padova</li>
  <li ><a href="talks/embeddingBeyond.pdf">Word embedding and the beyond.</a>> IR group, University of Padova</li>
  <li ><a href="talks/DL.pdf">Deep Learning in language</a> : offline workshop in Padova </li>
<!--   <li ><a href="talks/discussion.pdf">Research discussion</a> : Quartz project, University of Padova, Italy, 2018 Oct. </li>
  <li ><a href="talks/quartz_mid_improved.pdf">Individual Research Project for Quartz</a> : Quartz project, University of Padova, Italy, 2018 Oct. </li>
  <li ><a href="talks/quartz_qnn.pdf">Interpretable Neural network driven byquantum probability theory</a> : Quartz project, Germany. 2018 Sep. </li>
  <li ><a href="talks/tju.pdf">Exploring Interpretable Quantum Representation for language understanding</a> : Tianjin University, China. 2018 Sep. </li>
  <li ><a href="talks/tencent.pdf">Exploring Interpretable Quantum Representation for language understanding</a> : Tencent, China. 2018 Sep. </li>
  <li ><a href="talks/QuantumInterpretableNN.pdf">Exploring Interpretable Neural Network by Quantum representation</a> : Quartz workshop in Iatly, Padova. 2018 Sep. </li>
  <li ><a href="talks/overview.pdf">Representations and their matching: an overview of my previous research</a> : Padova, Italy. 2018.7 </li>
  <li ><a href="talks/textzoo.pdf">TextZOO, a new Benchmark to Reconsidering Text Classification</a> : Data Center, SNG, Tencent, Shenzhen, China. 2018.3.29 </li>
  <li ><a href="talks/nnqlm.pdf">Neural Network based Quantum Language Model for QA</a> : "AAAI 2018 Spotlights Proseminar ", Tencent, Shenzhen, China. 2018.3.28 </li>
  <li ><a href="talks/quantumnn.pdf">Quantum-inspired Neural Network</a> : <a href="http://www.quartz-itn.eu/training/winter-school">Quartz Winter School 2018 </a>, Padova, Italy 2018.2.14</li>
  <li ><a href="talks/chatbot.pdf">ChatBot in DSNO </a>: Apartment of product for instant communication, SNG, Tencent, Shenzhen, China. </li>
  <li ><a href="talks/quantumqa.pdf">Quantum Language Model for QA </a>: Quantum Penguin, Tencent, to be appeared </li>
  <li >Detail of our ChatBot : Apartment of product for instant communication, SNG, Tencent </li>
  <li >Progress of our ChatBot : the only non-leader 15-min Speaker in Center of Data Application, SNG, Tencent </li> -->
</ol>
<h3 style="margin: 0.0px 0.0px 0.0px 0.0px; text-align: justify; line-height: 17.0px; font: 14.0px Times; color: #000000; -webkit-text-stroke: #000000"><b>Teaching</b></h3>
  <ol class="ol1">
  <li ><a href="https://llm-course.github.io/">CSC 6201/CIE 6021 Large Language Models, Fall 2023/2024. (This might be the first course of large language models in the world.)</li>
  <li> <a href="https://nlp-course-cuhksz.github.io/"> CSC6052/DDA6307/MDS6002 Natural Language Processing, Spring 2024/2025</a> </li>
</ol>

  
<!-- <!-- <h3 style="margin: 0.0px 0.0px 0.0px 0.0px; text-align: justify; line-height: 17.0px; font: 14.0px Times; color: #000000; -webkit-text-stroke: #000000"><b>Industrial Projects</b></h3>
<ol class="ol1">
  <li ><a href="https://cloud.tencent.com/product/xiaowei">Chatbot: Customer Service Robot </a>in Tencent Cloud. 2017. With many traditional IR (information retrieval ) practice and deep learning application. I am the main designer and coder of the second generation customer service in Tencent Cloud, which has served more than 50 enterprises e.g. The Bank of China, Travel Bureau in Yunnan province, WeBank and Xinren Doctor.</li>
  <li >Modelling short-term interest user-profile for <a href="https://qzone.qq.com/">Qzone short </a>vedio recommendation.</li>
  <li >CNN-based language model in <a href="https://www.sogou.com/">Sogou</a> <a href="https://pinyin.sogou.com/">input method</a>, the most popular Chinese Input Method. </li>
</ol> -->
<h3 style="margin: 0.0px 0.0px 0.0px 0.0px; text-align: justify; line-height: 17.0px; font: 14.0px Times; color: #000000; -webkit-text-stroke: #000000"><b>Open-Source code</b></h3>
<ol class="ol1">
  <li> Phoenix <a class="github-button"
   href="https://github.com/FreedomIntelligence/LLMZoo"
   data-icon="octicon-star"
   data-size="large"
   data-show-count="true"
   aria-label="Star ntkme/github-buttons on GitHub">Star</a> </li>
  <li> Medical_NLP <a class="github-button"
   href="https://github.com/FreedomIntelligence/Medical_NLP"
   data-icon="octicon-star"
   data-size="large"
   data-show-count="true"
   aria-label="Star ntkme/github-buttons on GitHub">Star</a> </li>
  <li> HuatuoGPT <a class="github-button"
   href="https://github.com/FreedomIntelligence/HuatuoGPT"
   data-icon="octicon-star"
   data-size="large"
   data-show-count="true"
   aria-label="Star ntkme/github-buttons on GitHub">Star</a> </li>
  <li> TextClassificationBenchmark <a class="github-button"
   href="https://github.com/FreedomIntelligence/TextClassificationBenchmark"
   data-icon="octicon-star"
   data-size="large"
   data-show-count="true"
   aria-label="Star ntkme/github-buttons on GitHub">Star</a> </li>
  <li> InstructionZoo <a class="github-button"
   href="https://github.com/FreedomIntelligence/InstructionZoo"
   data-icon="octicon-star"
   data-size="large"
   data-show-count="true"
   aria-label="Star ntkme/github-buttons on GitHub">Star</a> </li>
  <li> HuatuoGPT-II <a class="github-button"
   href="https://github.com/FreedomIntelligence/HuatuoGPT-II"
   data-icon="octicon-star"
   data-size="large"
   data-show-count="true"
   aria-label="Star ntkme/github-buttons on GitHub">Star</a> </li>
  <li> ALLaVA <a class="github-button"
   href="https://github.com/FreedomIntelligence/ALLaVA"
   data-icon="octicon-star"
   data-size="large"
   data-show-count="true"
   aria-label="Star ntkme/github-buttons on GitHub">Star</a> </li>
  <li> Huatuo-26M <a class="github-button"
   href="https://github.com/FreedomIntelligence/Huatuo-26M"
   data-icon="octicon-star"
   data-size="large"
   data-show-count="true"
   aria-label="Star ntkme/github-buttons on GitHub">Star</a> </li>
  <li> crosstalk-generation <a class="github-button"
   href="https://github.com/FreedomIntelligence/crosstalk-generation"
   data-icon="octicon-star"
   data-size="large"
   data-show-count="true"
   aria-label="Star ntkme/github-buttons on GitHub">Star</a> </li>
  <li> ReasoningNLP <a class="github-button"
   href="https://github.com/FreedomIntelligence/ReasoningNLP"
   data-icon="octicon-star"
   data-size="large"
   data-show-count="true"
   aria-label="Star ntkme/github-buttons on GitHub">Star</a> </li>
  <li> Apollo <a class="github-button"
   href="https://github.com/FreedomIntelligence/Apollo"
   data-icon="octicon-star"
   data-size="large"
   data-show-count="true"
   aria-label="Star ntkme/github-buttons on GitHub">Star</a> </li>
  <li> Evaluation-of-ChatGPT-on-Information-Extraction <a class="github-button"
   href="https://github.com/FreedomIntelligence/Evaluation-of-ChatGPT-on-Information-Extraction"
   data-icon="octicon-star"
   data-size="large"
   data-show-count="true"
   aria-label="Star ntkme/github-buttons on GitHub">Star</a> </li>
  <li> CMB <a class="github-button"
   href="https://github.com/FreedomIntelligence/CMB"
   data-icon="octicon-star"
   data-size="large"
   data-show-count="true"
   aria-label="Star ntkme/github-buttons on GitHub">Star</a> </li>
  <li> OVM <a class="github-button"
   href="https://github.com/FreedomIntelligence/OVM"
   data-icon="octicon-star"
   data-size="large"
   data-show-count="true"
   aria-label="Star ntkme/github-buttons on GitHub">Star</a> </li>
  <li> AceGPT <a class="github-button"
   href="https://github.com/FreedomIntelligence/AceGPT"
   data-icon="octicon-star"
   data-size="large"
   data-show-count="true"
   aria-label="Star ntkme/github-buttons on GitHub">Star</a> </li>
  <li> MLLM-Bench <a class="github-button"
   href="https://github.com/FreedomIntelligence/MLLM-Bench"
   data-icon="octicon-star"
   data-size="large"
   data-show-count="true"
   aria-label="Star ntkme/github-buttons on GitHub">Star</a> </li>
  <li> PlatoLM <a class="github-button"
   href="https://github.com/FreedomIntelligence/PlatoLM"
   data-icon="octicon-star"
   data-size="large"
   data-show-count="true"
   aria-label="Star ntkme/github-buttons on GitHub">Star</a> </li>
<!--   <li ><a href="https://github.com/iclr-complex-order/complex-order">Word Wave</a>, Encoding word order in complex embeddings. </li>
  <li ><a href="https://github.com/wabyking/qnn">CNM</a>, Complex-valued neural network for NLP. </li>
  <li ><a href="https://github.com/geek-ai/irgan">IRGAN</a>, Generative Adversarial Nets for Question Answering. <a href="https://github.com/wabyking/IRGAN-AnswerSelection">A cleaner version</a> </li>
  <li ><a href="https://github.com/wabyking/TextClassificationBenchmarkInPytorch">TextZoo</a>, a new Text Classification Benchmark in PyTorch : in progress</li>
  <li ><a href="https://github.com/TJUIRLAB/NNQLM">NNQLM</a>, an End2end Quantum Language Model for Question Answering in Theano version. <a href="https://github.com/shuishen112/quantum-qa">Tensorflow version</a> </li>
  <li ><a href="https://github.com/wabyking/PyQLM">PyQLM </a>, A lightweight python project to implement Quantum Language model.</li>
  <li ><a href="https://github.com/wabyking/QuantumAttentionLSTM">QALSTM</a>, a LSTM with a Quantum parameter-free attention, in Theano.</li>
  <li ><a href="https://github.com/complexembedding/complex_word_embedding">CWE</a>, Complex word embedding, in Keras.</li>
  <li ><a href="https://github.com/wabyking/Gated_CNN_for_language_Modeling">CNN Language Model </a>, multi-GPU version in TensorFlow.</li> -->
</ol>
<!-- <h3 style="margin: 0.0px 0.0px 0.0px 0.0px; text-align: justify; line-height: 17.0px; font: 14.0px Times; color: #000000; -webkit-text-stroke: #000000"><b>Co-supervised Junior Students</b></h3>
<ol class="ol1">
  <li> Fei Yu, Phd,  </li>
  <li> Zhengyang Tang, Phd </li>
  <li> Junying Chen, Phd </li>
  <li> Xidong Wang, Phd </li>
  <li> Juhao Liang, Phd </li>
  <li >Mr. Niu Jiabing in Tianjin University. (now in Mobvoi)</li>
  <li >Miss. Zhang Shengnan in Tianjin University. (now in Toutiao)</li>
  <li >Miss. Niu Xiaolei in Tianjin University. (now in China mobile research institution)</li>
  <li >Mr. Su Zhan in Tianjin University. (now in Tencent)</li>
  <li >Mr. Zhang Lipeng in Tianjin University. (now in Hikvision research institution)</li>
  <li >Mr. Liqun Ma in Tianjin University. (now in Shenzhen Institutes of Advanced Technology (SIAT), Chinese Academy of Sciences)</li>
  <li >Mr. Donghao Zhao in Tianjin University. </li>
  <li >Miss. Xiaoliu Mao in Tianjin University. </li> 
</ol> --> 
</body>
</html>
